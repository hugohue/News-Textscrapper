{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News-TextScrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPed1/SbAD2QvLITLzIRNhc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugohue/News-Textscrapper/blob/master/News_TextScrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xahA9a25KkGF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **News Textscrapper with Beautifulsoup**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrArE-UdK37a",
        "colab_type": "text"
      },
      "source": [
        "This notebook demonstrates how to use beautifulsoup to capture the text of the top news from different online newspapaer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqNwdJZ5KbST",
        "colab_type": "text"
      },
      "source": [
        "Install Beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0a23W4VKKjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33b2e247-7572-4bcd-f83f-08c97eea508a"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swRtEyKvt6en",
        "colab_type": "text"
      },
      "source": [
        "Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV6mL3AILKBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import bs4 as bs\n",
        "import requests\n",
        "import urllib.request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osg9nfRVMOOV",
        "colab_type": "text"
      },
      "source": [
        "Capture the title and context for each top news from Fox News https://www.vox.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CvoCx9oMWBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init source\n",
        "ori_url = 'https://www.vox.com/'\n",
        "headers = {'User-Agent': 'Chrome/41.0.2228.0'}\n",
        "\n",
        "# Make request to the URL\n",
        "html = requests.get(ori_url, headers=headers)\n",
        "\n",
        "# Parse HTML and save to BeautifulSoup object¶\n",
        "soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "#print(soup)\n",
        "\n",
        "# get title from specific section\n",
        "print(\"Main News\")\n",
        "for paragraph in soup.find_all(\"div\", attrs = {\"class\" : \"c-newspaper__column\"}):\n",
        "  #print(\"a:\", paragraph.find_all(\"a\"))\n",
        "  for title in paragraph.find_all(\"a\", attrs = {\"data-analytics-link\" : \"article\"}):\n",
        "    print(\"Title: \",str(title.text))\n",
        "    print(\"url: \", title.get('href'))\n",
        "    url = title.get('href')\n",
        "    html = requests.get(url, headers=headers)\n",
        "    new_soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "    for external in new_soup.find_all(\"div\", attrs = {\"class\" : \"c-entry-content\"}):\n",
        "      print(\"Context: \")\n",
        "      for text in external(\"p\"):\n",
        "        print(str(text.text).strip())\n",
        "  for title in paragraph.find_all(\"a\", attrs = {\"data-analytics-link\" : \"feature\"}):\n",
        "    print(\"Title: \",str(title.text))\n",
        "    print(\"url: \", title.get('href'))\n",
        "    url = title.get('href')\n",
        "    html = requests.get(url, headers=headers)\n",
        "    new_soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "    for external in new_soup.find_all(\"div\", attrs = {\"class\" : \"c-entry-content\"}):\n",
        "      print(\"Context: \")\n",
        "      for text in external(\"p\"):\n",
        "        print(str(text.text).strip())\n",
        "print(\"Column News\")\n",
        "for paragraph in soup.find_all(\"div\", attrs = {\"class\" : \"l-col__main\"}):\n",
        "  for title in paragraph.find_all(\"a\", attrs = {\"data-analytics-link\" : \"article\"}):\n",
        "    print(\"Title: \",str(title.text))\n",
        "    print(\"url: \", title.get('href'))\n",
        "    url = title.get('href')\n",
        "    html = requests.get(url, headers=headers)\n",
        "    new_soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "    for external in new_soup.find_all(\"div\", attrs = {\"class\" : \"c-entry-content\"}):\n",
        "      print(\"Context: \")\n",
        "      for text in external(\"p\"):\n",
        "        print(str(text.text).strip())\n",
        "for paragraph in soup.find_all(\"div\", attrs = {\"class\" : \"l-col__main\"}):\n",
        "  for title in paragraph.find_all(\"a\", attrs = {\"data-analytics-link\" : \"feature\"}):\n",
        "    print(\"Title: \",str(title.text))\n",
        "    print(\"url: \", title.get('href'))\n",
        "    url = title.get('href')\n",
        "    html = requests.get(url, headers=headers)\n",
        "    new_soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "    for external in new_soup.find_all(\"div\", attrs = {\"class\" : \"c-entry-content\"}):\n",
        "      print(\"Context: \")\n",
        "      for text in external(\"p\"):\n",
        "        print(str(text.text).strip())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZJSfmF3LUa1",
        "colab_type": "text"
      },
      "source": [
        "Capture the title and context for each top news from http://www.yomiuri.co.jp/ 読売新聞オンライン"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3aAVqJCLKrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init source\n",
        "source = urllib.request.urlopen('http://www.yomiuri.co.jp/').read()\n",
        "soup = bs.BeautifulSoup(source,'lxml')\n",
        "# get title from specific section\n",
        "for paragraph in soup.find_all(\"ul\", attrs = {\"class\" : \"p-category-latest-sec-list p-list p-list-col2\"}):\n",
        "  #print(\"a:\", paragraph.find_all(\"a\"))\n",
        "  for title in paragraph.find_all(\"a\"):\n",
        "    print(\"Title: \",str(title.text))\n",
        "    print(\"URL: \",title.get('href'))\n",
        "    #print(type(title.get('href')))\n",
        "    # change the source with the captured url\n",
        "    source = urllib.request.urlopen(title.get('href')).read()\n",
        "    soup = bs.BeautifulSoup(source,'lxml')\n",
        "    #print(\"div: \", soup.find_all(\"div\", attrs = {\"class\" : \"p-main-contents\"}))\n",
        "    # get the context from the captured source\n",
        "    for in_div in soup.find_all(\"div\", attrs = {\"class\" : \"p-main-contents\"}):\n",
        "      #print(\"p: \", in_div.find_all(\"p\"))\n",
        "      print(\"Context:\")\n",
        "      for in_text in in_div.find_all(\"p\"):\n",
        "        #print(\"Context: \", str(in_text.text))\n",
        "        print(str(in_text.text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbbyVzsmL8cN",
        "colab_type": "text"
      },
      "source": [
        "Capture the title and context for each top news from https://www.thestandnews.com 立場新聞\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eJa7BYnMFX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init source\n",
        "ori_url = 'https://www.thestandnews.com'\n",
        "headers = {'User-Agent': 'Chrome/41.0.2228.0'}\n",
        "\n",
        "\n",
        "# Make request to the URL\n",
        "html = requests.get(ori_url, headers=headers)\n",
        "\n",
        "# Parse HTML and save to BeautifulSoup object¶\n",
        "soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "#print(soup)\n",
        "#source = urllib.request.urlopen('https://www.thestandnews.com/').read()\n",
        "#soup = bs.BeautifulSoup(source,'lxml')\n",
        "# get title from specific section\n",
        "for paragraph in soup.find_all(\"div\", attrs = {\"class\" : \"page-content\"}):\n",
        "  #print(\"a:\", paragraph.find_all(\"a\"))\n",
        "  for title in paragraph.find_all(\"h3\"):\n",
        "    print(\"Title: \",str(title.text))\n",
        "    for url_plus in title.find_all(\"a\"):\n",
        "      print(\"URL: \",url_plus.get('href'))\n",
        "      url = ori_url + url_plus.get('href')\n",
        "      print(\"url\", url)\n",
        "      html = requests.get(url, headers=headers)\n",
        "      url = ''\n",
        "      soup = BeautifulSoup(html.text, \"html.parser\")\n",
        "      for external_url in soup.find_all(\"div\", attrs = {\"class\" : \"article-content\"}):\n",
        "        print(\"Context: \")\n",
        "        for text in external_url.find_all(\"p\"):\n",
        "          print(str(text.text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMgOECNthIJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}